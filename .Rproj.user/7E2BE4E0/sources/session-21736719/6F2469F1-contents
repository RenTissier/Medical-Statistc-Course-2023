---
title: "Qol-AI-previousdata"
author: "Renaud"
date: '2022-11-29'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r}
#' @title cv.glmmLasso
#' @description Does k-fold cross validation for glmmLasso  
#' @details Build multiple models given a sequence of lambda values
#' @author Pirapong Jitngamplang, Jared Lander
#' @export
#' @importFrom dplyr "%>%"
#' @param fix A two-sided linear formula object describing the fixed-effects part of the model, with the response on the left of a ~ operator and the terms, separated by + operators, on the right. For categorical covariables use as.factor(.) in the formula. Note, that the corresponding dummies are treated as a group and are updated blockwise
#' @param rnd A two-sided linear formula object describing the random-effects part of the model, with the grouping factor on the left of a ~ operator and the random terms, separated by + operators, on the right; aternatively, the random effects design matrix can be given directly (with suitable column names). If set to NULL, no random effects are included.
#' @param data The data frame containing the variables named in formula.
#' @param family A GLM family, see [glm()] and [family()]. Also ordinal response models can be fitted: use family=acat() and family=cumulative() for the fitting of an adjacent category or cumulative model, respectively. If family is missing then a linear mixed model is fit; otherwise a generalized linear mixed model is fit.
#' @param kfold Number of folds - default is 10. Although k-folds can be as large as the sample size (leave-one-out CV), it is not recommended for large datasets. Smallest value allowable is nfolds = 3
#' @param lambdas Optional user-supplied lambda sequence; default is NULL, and glmmLasso_MultLambdas chooses its own sequence
#' @param nlambdas The number of lambdas values, default value is 100 if lambdas is not user-supplied
#' @param lambda.min.ratio Smallest value for lambda, as a fraction of lambda.max, the (data derived) entry value (i.e. the smallest value for which all coefficients are zero). The default depends on the sample size nobs relative to the number of variables nvars. If nobs > nvars, the default is 0.0001, close to zero. If nobs < nvars, the default is 0.01.
#' @param loss Loss function used to calculate error, default values is based on family: \cr
#' 
#' \itemize{
#' \item gaussian = [cv.glmmLasso::calc_mse()] 
#' \item binomial = [cv.glmmLasso::calc_logloss()] 
#' \item multinomial = [cv.glmmLasso::calc_multilogloss()] 
#' \item poisson = [cv.glmmLasso::calc_deviance()]
#'   }
#' 
#' @param lambda.final Choice for final model to use lambda.1se or lambda.min, default is lambda.1se
#' @param \dots can receive parameters accepted by glmmLasso
#' @md
#' @return A list of cross-validation values including: \cr 
#' 
#' 
#' \describe{
#' \item{lambdas}{The values of lambda used in the fits} 
#' \item{cvm}{The mean cross-validated error - a vector of length length(lambda)} 
#' \item{cvsd}{Estimate of standard error of cvm.}
#' \item{cvup}{Upper curve = cvm+cvsd.}
#' \item{cvlo}{Lower curve = cvm-cvsd.} 
#' \item{glmmLasso.final}{A fitted glmmLasso object for the full data} 
#' \item{lambda.min}{Value of lambda that gives minimum cvm} 
#' \item{lambda.1se}{Largest value of lambda such that error is within 1 standard error of the minimum}
#' }
#' 
#' 
#' @examples 
#' data("soccer", package = "glmmLasso")
#' soccer[,c(4,5,9:16)]<-scale(soccer[,c(4,5,9:16)],center=TRUE,scale=TRUE)
#' soccer <- data.frame(soccer)
#' 
#' mod1 <- cv.glmmLasso(fix = points ~ transfer.spendings + ave.unfair.score + 
#' ball.possession + tackles, rnd = list(team=~1), data = soccer, 
#' family = gaussian(link = "identity"), kfold = 5, lambda.final = 'lambda.1se')


cv.glmmLasso <- function(fix, rnd, data, 
                         family = stats::gaussian(link = "identity"), 
                         kfold = 5, lambdas = NULL, nlambdas = 100, 
                         lambda.min.ratio = ifelse(nobs < nvars, 0.01, 0.0001), 
                         loss,
                         lambda.final=c('lambda.1se', 'lambda.min'),
                         ...)
{
    lambda.final <- match.arg(lambda.final)
    
    if(missing(loss))
    {
        # switch allows us to do take the family arg as assign the appropriate 
        # loss function 
        loss <- switch(family$family, 
                       'gaussian' = calc_mse,
                       'binomial' = calc_logloss,
                       'multinomial' = calc_multilogloss,
                       'poisson' = calc_deviance)
    }
    
    x <- useful::build.x(fix, data)
    nobs <- nrow(x)
    nvars <- ncol(x)
    
    # if lambda isn't specified by user, build the lambdas vector, this is 
    # static for all k folds
    if (is.null(lambdas))
    {
        # building the lambda vector
        lambdas <- buildLambdas(fix = fix,
                                rnd = rnd,
                                data = data, 
                                nlambdas = nlambdas, 
                                lambda.min.ratio= lambda.min.ratio)   
    }
    
    
    
    
    # building data frame to map a specific row to kth group
    # column 1 is the row, column 2 is a randomly assigned group
    # number of groups is determined by kfold value  
    rowDF <- tibble::tibble(
        row = seq(nobs),
        group = sample(rep(seq(kfold), length.out=nobs), replace = FALSE)
    )
    
    # sorting by group 
    rowDF <-  dplyr::arrange(rowDF, .data$group)
    
    
    #instantiating list to hold loss and models for each fold
    lossVecList <- vector(mode = 'list', length = kfold)
    modList_foldk <- vector(mode = 'list', length = kfold)
    
    for(k in 1:kfold)
    {
        testIndices <- dplyr::filter(rowDF, .data$group == k) %>% dplyr::pull(row)
        trainIndices <- rowDF$row[-testIndices]
        
        # fitting model
        # modList_foldk is a glmmLasso_MultLambdas object, which is a list of 
        # glmmLasso objects
        
        # for showing lambda at each iterations
        # message(sprintf('Round: %s\n ', k))
        modList_foldk[[k]] <- glmmLasso_MultLambdas(fix = fix,
                                                    rnd = rnd,
                                                    data = data %>% dplyr::slice(trainIndices),
                                                    family = family,
                                                    lambdas = lambdas,
                                                    nlambdas = nlambdas,
                                                    lambda.min.ratio = lambda.min.ratio,
                                                    ...)
        
        
        
        # hacky way of getting the response variable out of the         
        response_var <- fix[[2]] %>% as.character()
        
        # pulling out actual data
        actualDataVector <- data %>% dplyr::slice(testIndices) %>% 
            dplyr::pull(response_var)
        
        # predicting values for each of the glmmLasso model (100 lambda) 
        # using matrix form for easier error calculation in loss()
        
        predictionMatrix <- predict.glmmLasso_MultLambdas(
            object = modList_foldk[[k]],
            newdata = data %>% dplyr::slice(testIndices)
        )
        
        # employing the loss function in form loss(actual,predicted)
        # using loss function, calculating a list of loss values for each vector 
        # of prediction
        # which comes from a glmmLasso model with a specific lambda 
        # storing loss values for each fold
        
        # TODO: think an error is thrown here 
        lossVecList[[k]] <- loss(actual = actualDataVector, predicted = predictionMatrix)
        # each element of this list should be 1 x nlambdas
    }
    
    #building matrix (k by nlambdas) to help calculate cross-validated mean error
    cvLossMatrix <- do.call(what = rbind, args = lossVecList)
    
    cvm = colMeans(cvLossMatrix)
    
    # calculating sd, cv, up, down
    cvsd <- apply(cvLossMatrix, 2, stats::sd, na.rm = TRUE)
    cvup <- cvm + cvsd
    cvlo <- cvm - cvsd
    
    
    # finding the minimum cvm value in order pull out the lambda.min out of 
    # list of lambda
    minIndex <- which.min(cvm)    
    lambda.min <- lambdas[minIndex]
    
    # finding 1se index by doing vectorized comparision such that cvm <= cvup 
    # of minIndex
    my1seIndex <- min(which(cvm <= cvup[minIndex]))
    lambda.1se <- lambdas[my1seIndex]
    
    # chosing lambda.final to use by checking lambda.final option
    # note that first element lambda.final default value will return true for
    # lambda.1se 
    chosenLambda <- if(lambda.final == 'lambda.1se')
    {
        lambda.1se
    }else if(lambda.final == 'lambda.min')
    {
        lambda.min
    }
    
    
    
    glmmLasso.final <- glmmLasso::glmmLasso(fix = fix,
                                            rnd = rnd,
                                            data = data,
                                            family = family,
                                            lambda = chosenLambda)
    
    # add control list argument to this to make converge faster form one that 
    # create lambda.1se
    # TODO: (maybe) For final model fit, supply control list from the model that led to     either lambda.1se or lambda.min
    
    # mimicking cv.glmnet return objects
    return_List <- list(lambdas=lambdas,
                        cvm=cvm,
                        cvsd=cvsd,
                        cvup=cvup,
                        cvlo=cvlo,
                        glmmLasso.final=glmmLasso.final,
                        lambda.min=lambda.min,
                        lambda.1se=lambda.1se)
    
    
    class(return_List) <- 'cv.glmmLasso'
    
    
    return(return_List)
    
}


# modified from calc.deviance in dismo package - credit to Robert Hijmans

#' 
#' @title calc_mse
#' @description Function for calculating mse
#' @details Loss functions written for use in cv.glmmLasso 
#' @author Pirapong Jitngamplang, Jared Lander
#' @param actual actual data values 
#' @param predicted predicted data values
#' @return error between actual versus prediction
#'

calc_mse <- function(actual, predicted)
{
    return(colMeans((actual - predicted)^2)) 
}

#' 
#' @title calc_logloss
#' @description Functions for calculating logloss
#' @details Loss functions written for use in cv.glmmLasso 
#' @author Pirapong Jitngamplang, Jared Lander
#' @param actual actual data values 
#' @param predicted predicted data values
#' @return error between actual versus prediction
#'
calc_logloss <- function(actual, predicted)
{
    
    score <- -(actual * log(predicted) + (1 - actual) * log(1 -predicted))
    score[actual == predicted] <- 0
    score[is.nan(score)] <- Inf
    return(colMeans(score))
    
}

#' 
#' @title calc_multilogloss
#' @description Function for calculating multilogloss
#' @details loss functions written for use in cv.glmmLasso 
#' @author Pirapong Jitngamplang, Jared Lander
#' @param actual actual data values 
#' @param predicted predicted data values
#' @return error between actual versus prediction
#'

# modified from MultiLogLoss in MLMetrics package - credit to Yachen Yan

calc_multilogloss <- function(actual, predicted) 
{
    return(apply(predicted, 2, MLmetrics::MultiLogLoss, y_true = actual)) 
}


#' 
#' @title calc_deviance
#' @description Functions for calculating deviance
#' @details loss functions written for use in cv.glmmLasso 
#' @author Pirapong Jitngamplang, Jared Lander
#' @param actual actual data values 
#' @param predicted predicted data values
#' @param family default value is poisson
#' @param \dots can receive parameters accepted by dismo::calc.deviance
#' @return error between actual versus prediction
#'
calc_deviance <- function(actual, predicted, family = 'poisson',...)
{
    
    return(apply(predicted, 2, dismo::calc.deviance, obs = actual, family = family,
                 ...))
}

# this function compute the max lambda based on formula given

#' @title computeLambdaMax
#' @description compute the maximum lambda value based on given dataset 
#' @details lambdaMax is computed based on  the coordinate descent algorithm from this paper: Friedman, Jerome, Trevor Hastie, and Rob Tibshirani. "Regularization paths for generalized linear models via coordinate descent."
#' @author Pirapong Jitngamplang, Jared Lander
#' @param fix A two-sided linear formula object describing the fixed-effects part of the model, with the response on the left of a ~ operator and the terms, separated by + operators, on the right. For categorical covariables use as.factor(.) in the formula. Note, that the corresponding dummies are treated as a group and are updated blockwise
#' @param rnd A two-sided linear formula object describing the random-effects part of the model, with the grouping factor on the left of a ~ operator and the random terms, separated by + operators, on the right; aternatively, the random effects design matrix can be given directly (with suitable column names). If set to NULL, no random effects are included.
#' @param data The data frame containing the variables named in formula.
#' @param scale default value is true
#' @return returns the lambdaMax value based on given dataset


computeLambdaMax <- function(fix, rnd, data, scale=TRUE)
{
    # converting formula into matrices to do lambdaMax calculation
    y <- useful::build.y(fix, data)
    x <- useful::build.x(fix, data)
    
    if(scale)
    {
        x <- scale(x)
    }
    
    # exp because of log scale
    # N*alpha*lambdaMax = max_l(<x_l, y>)
    lambdaMax <- exp(max(abs(colSums(x*y)), na.rm=TRUE) / nrow(data))
    
    # colSums(x*y) is same as crossprod(x,y)
    
    return(lambdaMax)
}

#' @title buildLambdas
#' @description generate lambda vector based on dataset given
#' @author Pirapong Jitngamplang, Jared Lander
#' @param fix A two-sided linear formula object describing the fixed-effects part of the model, with the response on the left of a ~ operator and the terms, separated by + operators, on the right. For categorical covariables use as.factor(.) in the formula. Note, that the corresponding dummies are treated as a group and are updated blockwise
#' @param rnd A two-sided linear formula object describing the random-effects part of the model, with the grouping factor on the left of a ~ operator and the random terms, separated by + operators, on the right; alternatively, the random effects design matrix can be given directly (with suitable column names). If set to NULL, no random effects are included.
#' @param data The data frame containing the variables named in formula.
#' @param nlambdas the number of lambdas values, default value is 100 if lambdas is not user-supplied
#' @param lambda.min.ratio Smallest value for lambda, as a fraction of lambda.max, the (data derived) entry value (i.e. the smallest value for which all coefficients are zero). The default depends on the sample size nobs relative to the number of variables nvars. If nobs > nvars, the default is 0.0001, close to zero. If nobs < nvars, the default is 0.01.
#' @return returns a vector of lambda
#'


buildLambdas <- function(fix, rnd, data, 
                         nlambdas = 100, 
                         lambda.min.ratio = ifelse(nobs < nvars, 0.01, 0.0001))
{
    # converting formula into matrices to do lambdaMax calculation
    x <- useful::build.x(fix, data)
    nobs <- nrow(x)
    nvars <- ncol(x)
    
    lambdaMax = computeLambdaMax(fix = fix, 
                                 rnd = rnd,
                                 data = data)
    
    lambda_vec <- seq(from = lambdaMax, 
                      to = lambdaMax * lambda.min.ratio, 
                      length.out = nlambdas) 
    # sorting such that first lambda is the largest
    lambda_vec <- sort(lambda_vec, decreasing = TRUE)
    
    return(lambda_vec)
}


#' @title glmmLasso_MultLambdas
#' @description Variable selection using glmmLasso for multiple lambdas values  
#' @details Build multiple models given a sequence of lambda values
#' @author Pirapong Jitngamplang, Jared Lander
#' @export
#' @param fix A two-sided linear formula object describing the fixed-effects part of the model, with the response on the left of a ~ operator and the terms, separated by + operators, on the right. For categorical covariables use as.factor(.) in the formula. Note, that the corresponding dummies are treated as a group and are updated blockwise
#' @param rnd A two-sided linear formula object describing the random-effects part of the model, with the grouping factor on the left of a ~ operator and the random terms, separated by + operators, on the right; aternatively, the random effects design matrix can be given directly (with suitable column names). If set to NULL, no random effects are included.
#' @param data The data frame containing the variables named in formula.
#' @param family a GLM family, see glm and family. Also ordinal response models can be fitted: use family=acat() and family=cumulative() for the fitting of an adjacent category or cumulative model, respectively. If family is missing then a linear mixed model is fit; otherwise a generalized linear mixed model is fit.
#' @param lambdas The penalty parameter that controls the shrinkage of fixed terms and controls the variable selection. The optimal penalty parameter is a tuning parameter of the procedure that has to be determined, e.g. by use of information criteria or cross validation. Should inputted as a numeric vector from high to low. (See details for an example.)
#' @param nlambdas the number of lambdas values, default value is 100.
#' @param lambda.min.ratio Smallest value for lambda, as a fraction of lambda.max, the (data derived) entry value (i.e. the smallest value for which all coefficients are zero). The default depends on the sample size nobs relative to the number of variables nvars. If nobs > nvars, the default is 0.0001, close to zero. If nobs < nvars, the default is 0.01.
#' @param \dots can receive parameters accepted by glmmLasso
#' @return Returns a glmmLasso_MultLambdas object, which is list glmmLasso models for each lambda value.  
#' @examples
#' 
#' library(glmmLasso)
#' data("soccer")
#' 
#' mod1 <- glmmLasso_MultLambdas(fix = points ~ transfer.spendings + 
#' ball.possession + tackles , rnd = list(team =~ 1), 
#' data = soccer, family = poisson(link = log)) 
#' 
#' 
#'  

glmmLasso_MultLambdas <- function(fix, rnd, data, 
                                  family = stats::gaussian(link = "identity"), 
                                  lambdas = NULL,
                                  nlambdas = 100,
                                  lambda.min.ratio=ifelse(nobs < nvars, 0.01, 0.0001), 
                                  ...)
{
    
    # fitting first model to generate initial inputs for control parameter
    # here we use the first lambda (highest penalty) to start
    # based glmmLasso's author, glmmLasso is faster when final coefficient
    # estimates corresponding to a lambda is used as the starting value for
    # the next smaller lambda  
    
    # defining the number of observation
    nobs <- nrow(data)
    
    # defining the number of preditors based on the number of terms in fix formula
    nvars <- length(attr(stats::terms(fix), 'term.labels'))
    
    if (is.null(lambdas))
    {
        
        # building the lambda vector
        lambdas <- buildLambdas(fix = fix,
                                rnd = rnd,
                                data = data, 
                                nlambdas = nlambdas, 
                                lambda.min.ratio = lambda.min.ratio)    
    }
    
    
    
    # passing Q.start and Delta.start is modeled from glmmLasso demo file
    # from the "More Elegant section" 
    
    # Delta is matrix containing the estimates of fixed and random effects 
    # (columns) for each iteration (rows) of the main algorithm (i.e. before 
    # the final re-estimation step is performed, see details).
    # Passing the set of estimates from the last iteration as the 
    # 'start' parameter of the controlList
    
    # Q_long is a list containing the estimates of the random effects
    # variance-covariance parameters for each iteration of the main algorithm.
    # Passing the variance-covaiance matrix as the q_start parameter of
    # the controlList
    
    
    
    # initializing list of object to hold the model outputs 
    modList <- vector(mode = 'list', length = length(lambdas))
    
    
    # fit first lambda
    first_fit <- glmmLasso::glmmLasso(fix = fix,
                                      rnd = rnd,
                                      data = data,
                                      family = family,
                                      lambda = lambdas[1],
                                      ...)
    # builing the first Delta.start, transpose required to make dimension
    
    Delta.start <- first_fit$Deltamatrix[first_fit$conv.step, ] %>% t()
    Q.start <- first_fit$Q_long[[first_fit$conv.step + 1]]
    
    for (l in seq_along(lambdas))
    {
        
        # for showing lambda at each iteration
        # message(sprintf('Lambda: %s\n ', lambdas[l]))
        
        fit <- glmmLasso::glmmLasso(fix = fix,
                                    rnd = rnd,
                                    data = data,
                                    family = family,
                                    lambda = lambdas[l],
                                    control = list(start=Delta.start[l,],
                                                   q_start=Q.start[l]),...)
        
        # storing model objects before storing to modList
        fit$lambda <- lambdas[l]
        fit$Delta.start <- Delta.start[l,]
        fit$Q.start <- Q.start[l]
        fit$data <- data
        fit$rnd <- rnd
        fit$fix <- fix
        fit$family <- family
        
        modList[[l]] <- fit
        Delta.start <- rbind(Delta.start, fit$Deltamatrix[fit$conv.step, ])
        Q.start <- c(Q.start, fit$Q_long[[fit$conv.step + 1]])
        
        
        
    }
    
    # the function returns a list of glmmLasso models 
    
    attr(modList, 'lambdas') <- lambdas
    
    class(modList) <- 'glmmLasso_MultLambdas'
    
    return(modList)
}



predict.glmmLasso_MultLambdas <- function(object, newdata, ...)
{
    # instantiating list to hold nlambdas number of n x 1 vectors 
    # pred_vec_list <- vector(mode = 'list', length = length(object))
    # storing returned vectors in a list 
    
    pred_vec_list <- purrr::map(.x = object, .f = stats::predict, 
                                newdata = newdata)
    
    pred_matrix <- do.call(what = cbind, args = pred_vec_list)
    
    return(pred_matrix)
}
```

```{r}

data_qol <- read.delim('Update dataset Itske 27052022 -c30.txt')
data_abdomen <- read.delim('__final_c30_abdomen.txt')[,-c(1)] # I have transformed the csv file into a txt file to import directly the csv file use the function read.csv
data_abdomen <- data_abdomen[complete.cases(data_abdomen$VK_PATIENTNR),]
```

# Data preparation

## Preparation QoL dataset 

```{r}

physical_fun_raw <- rowMeans(data_qol[, c('q01','q02','q03','q04','q05')],na.rm=TRUE)
physical_fun_score <- 100*(1-(physical_fun_raw-1)/3)
role_fun_raw <- rowMeans(data_qol[, c('q06','q07')],na.rm=TRUE)
role_fun_score <- 100*(1-(role_fun_raw-1)/3)
fatigue_fun_raw <- rowMeans(data_qol[, c('q10','q12','q18')],na.rm=TRUE)
fatigue_fun_score <- 100*((fatigue_fun_raw-1)/3)

data_score <- data.frame(physical=physical_fun_score,role=role_fun_score, fatigue=fatigue_fun_score)

data_score <- cbind(data_qol[,c(1:3)],data_score)

data_difference <- NULL
t.prior <- NULL
prior <- NULL
subs <- NULL
id <- NULL
t.subs <- NULL
for (l in unique(data_score$Patient_nr)){
  data.patientqol <- data_score[data_score$Patient_nr==l,]
  if(length(data.patientqol[,1])>1){
      for (m in 2:length(data.patientqol[,1])){
    id <- c(id, l)
    prior <- c(prior, data.patientqol$Invuldt[(m-1)])
    t.prior <- c(t.prior, data.patientqol$Meetmoment[(m-1)])
    subs <- c(subs, data.patientqol$Invuldt[m])
    t.subs <- c(t.subs, data.patientqol$Meetmoment[m])
    data_difference <- rbind(data_difference, data.patientqol[m,4:6]- data.patientqol[(m-1),4:6])
    }
  }
}
hist(data_difference$physical)
hist(data_difference$role)
hist(data_difference$fatigue)

data.qol <- as.data.frame(cbind(id,prior,t.prior,subs,t.subs,data_difference))
data.qol$prior <- as.Date(data.qol$prior,'%d/%m/%Y')
data.qol$subs <- as.Date(data.qol$subs,'%d/%m/%Y')

```

## Combination of QoL and Radiomics dataset

```{r}
# obtention of the list of patients available in both dataset

patients.common <- intersect(unique(data.qol$id), unique(data_abdomen$VK_PATIENTNR))

```

We have `r length(patients.common)` common patients between the quality of life dataset and the ct abdomen dataset. We will now combine both datasets so that we keep only between ct scans if both previous scan date and subsequent scan data are within 6 weeks of quality of life measurements.

To do so we will first keep only radiology features for each patient that are within 6 weeks of filled questionnaires about quality of life. both for the scan prior and the susbsequent scan:


```{r}

data_abdomen$SCAN_PRIOR_DATE <- as.Date(data_abdomen$SCAN_PRIOR_DATE, format = '%d/%m/%Y')
data_abdomen$SCAN_SUBSQ_DATE <- as.Date(data_abdomen$SCAN_SUBSQ_DATE, format = '%d/%m/%Y')
data.radiol.filt <- NULL
t.prior <- NULL
t.subsq <- NULL
for (k in patients.common){
  data.radiology <- data_abdomen[data_abdomen$VK_PATIENTNR==k,]
  data.qol.patient <- data.qol[data.qol$id==k,]
  for (n in 1:length(data.radiology[,1])){
    dist.prior <- as.numeric(data.radiology$SCAN_PRIOR_DATE[n]) - as.numeric(data.qol.patient$prior)
    dist.subsq <- as.numeric(data.radiology$SCAN_SUBSQ_DATE[n]) - as.numeric(data.qol.patient$subs)
    if (min(abs(dist.prior))<= 42 & min(abs(dist.subsq))<= 42){
      data.radiol.filt <- rbind(data.radiol.filt,data.radiology[n,])
      t.prior <- c(t.prior, data.qol.patient$t.prior[which.min(abs(dist.prior))])
      t.subsq <- c(t.subsq, data.qol.patient$t.subs[which.min(abs(dist.subsq))])
    }
  }
}
data.radiology <- cbind(data.radiol.filt,t.prior, t.subsq)

data.feature <- data.radiology[,c('VK_PATIENTNR','t.prior','SCAN_PRIOR_DATE','t.subsq','SCAN_SUBSQ_DATE',names( data.radiology )[grepl( "feature" , names( data.radiology ) )])]


data.patient.strange <- data.feature[data.feature$t.prior >= data.feature$t.subsq,]
strange.patient <- unique(data.patient.strange$VK_PATIENTNR)

data.feature.filt <- data.feature[-which(data.feature$t.prior>= data.feature$t.subsq),]
data.feature.filt$time <- paste0(data.feature.filt$t.prior,data.feature.filt$t.subsq)
```

We have several scans per patient that are taken at the same dates with different ct scan parameters. For now we will average the radiomics features measured at the same time points but with different scanner parameters:

## Averaging

```{r}
data.rad.pred <- NULL
for (patient in unique(data.feature.filt$VK_PATIENTNR)){
  data.patient <- data.feature.filt[data.feature.filt$VK_PATIENTNR==patient,]
  for(tp in unique(data.patient$time)){
    data.patient.tp <- apply(data.patient[data.patient$time == tp,-which(colnames(data.patient) %in% c('SCAN_PRIOR_DATE','SCAN_SUBSQ_DATE','time'))],2,mean)
    data.rad.pred <- rbind(data.rad.pred,data.patient.tp)
  }
}
data.rad.pred <- as.data.frame(data.rad.pred)

```

# Features correlation

```{r}
library(reshape2)
library(ggplot2)
library(gplots)
features <- c(4:515)
cor.c30ab.ft <- cor(data.rad.pred[,features])

melted_cor <- melt(cor.c30ab.ft)

hist(melted_cor$value, main='histogram of correlations between features')

ggplot(data = melted_cor, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile()+
scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
   name="Pearson\nCorrelation") +
  theme_minimal()

length(melted_cor[melted_cor[,3]>0.99,1])
```
We can now merge quality of life and radiomics together:

## Merge


```{r,}
colnames(data.rad.pred)[c(1,3)]<- c('id','t.subs')

data.pred <- merge(data.qol, data.rad.pred, by = c('id','t.prior','t.subs') )
data.pred <-transform(data.pred, timepoint=paste(t.prior, t.subs))

data.pred[,521]
```

# Prediction

## Between time point 1 and 2

```{r}
library(gplots)
data.pred.12 <- data.pred[data.pred$timepoint=='1 2',]

hist(data.pred.23$fatigue)
hist(data.pred.23$role)
hist(data.pred.23$physical)

data.feature.12 <- data.pred.12[,grepl( "feature" , names( data.pred.12 ) )]

data.feature.12 <- data.feature.12[,which(apply(data.feature.12,2,sd)!=0)]
heatmap.2(cor(data.feature.12),trace='none',col= colorRampPalette(c("green","white","red"))(40))

```

We will now build the prediction model for the three quality of life score (physical, role and fatigue, namely). to do so we will use the function glmnet that uses ElasticNet in order to be the most precise prediction model. ElasticNet is a weighted linear combination regularized regression methods ridge (no variable selection deals well with correlated variables) and lasso (variable selection, not great for strongly correlated variables). We can change the weight ElasticNet denoted by the parameter alpha in the function to be ridge only (alpha = 0), lasso only (alpha = 1) or a mixed combination of both (alpha= 0.5 is giving the same weight to ridge and lasso). To avoid overfitting we will use cross-validation. The number of folds can be changed using the parameter nfold.

### physical

```{r}

library(glmnet)
model.lasso.cv <- cv.glmnet(x=as.matrix(data.feature.12), y=data.pred.12$physical , family='gaussian',type.measure='deviance',nfolds = 3,alpha = 1)

plot(model.lasso.cv)

train.model.lasso <-glmnet(x=as.matrix(data.feature.12), y=data.pred.12$physical, family='gaussian',type.measure='deviance',alpha = 1, lambda = model.lasso.cv$lambda.1se)

train.model.lasso$beta

```

## role

```{r}

library(glmnet)
model.lasso.cv <- cv.glmnet(x=as.matrix(data.feature.12[complete.cases(data.pred.12$role),]), y=data.pred.12$role[complete.cases(data.pred.12$role)] , family='gaussian',type.measure='deviance',nfolds = 3,alpha = 1)

plot(model.lasso.cv)

train.model.lasso <-glmnet(x=as.matrix(data.feature.12[complete.cases(data.pred.12$role),]), y=data.pred.12$role[complete.cases(data.pred.12$role)], family='gaussian',type.measure='deviance',alpha = 1, lambda = model.lasso.cv$lambda.1se)

train.model.lasso$beta

```

## fatigue

```{r}

library(glmnet)
model.lasso.cv <- cv.glmnet(x=as.matrix(data.feature.12[complete.cases(data.pred.12$fatigue),]), y=data.pred.12$fatigue[complete.cases(data.pred.12$fatigue)] , family='gaussian',type.measure='deviance',nfolds = 3,alpha = 1)

plot(model.lasso.cv)

train.model.lasso <-glmnet(x=as.matrix(data.feature.12)[complete.cases(data.pred.12$fatigue),], y=data.pred.12$fatigue[complete.cases(data.pred.12$fatigue)], family='gaussian',type.measure='deviance',alpha = 1, lambda = model.lasso.cv$lambda.1se)

train.model.lasso$beta

```


## Between time point 2 and 3

```{r}
data.pred.23 <- data.pred[data.pred$t.prior==2 & data.pred$t.subs==3,]
data.feature.23 <- data.pred.23[,grepl( "feature" , names( data.pred.23 ) )]
data.feature.23 <- data.feature.23[,which(apply(data.feature.23,2,sd)!=0)]
heatmap.2(cor(data.feature.23),trace='none',col= colorRampPalette(c("green","white","red"))(20))
```

### physical

```{r}

library(glmnet)
model.lasso.cv <- cv.glmnet(x=as.matrix(data.feature.23), y=data.pred.23$physical , family='gaussian',type.measure='deviance',nfolds = 3,alpha = 1)

plot(model.lasso.cv)

train.model.lasso <-glmnet(x=as.matrix(data.feature.23), y=data.pred.23$physical, family='gaussian',type.measure='deviance',alpha = 1, lambda = model.lasso.cv$lambda.1se)

train.model.lasso$beta

```

## role

```{r}

library(glmnet)
model.lasso.cv <- cv.glmnet(x=as.matrix(data.feature.23[complete.cases(data.pred.23$role),]), y=data.pred.23$role[complete.cases(data.pred.23$role)] , family='gaussian',type.measure='deviance',nfolds = 3,alpha = 1)

plot(model.lasso.cv)

train.model.lasso <-glmnet(x=as.matrix(data.feature.23[complete.cases(data.pred.23$role),]), y=data.pred.23$role[complete.cases(data.pred.23$role)], family='gaussian',type.measure='deviance',alpha = 1, lambda = model.lasso.cv$lambda.1se)

train.model.lasso$beta

```

## fatigue

```{r}

library(glmnet)
model.lasso.cv <- cv.glmnet(x=as.matrix(data.feature.23[complete.cases(data.pred.23$fatigue),]), y=data.pred.23$fatigue[complete.cases(data.pred.23$fatigue)] , family='gaussian',type.measure='deviance',nfolds = 3,alpha = 0.5)

plot(model.lasso.cv)

train.model.lasso <-glmnet(x=as.matrix(data.feature.23)[complete.cases(data.pred.23$fatigue),], y=data.pred.23$fatigue[complete.cases(data.pred.23$fatigue)], family='gaussian',type.measure='deviance',alpha = 0.5, lambda = model.lasso.cv$lambda.1se)

train.model.lasso$beta

```

# Longitudinal model

## physical

```{r}
library(fad)

library(Matrix)
###data.matrix <- as(as.matrix(data.pred.long[,c(3:514)]), "sparseMatrix")
###fad(x=data.matrix,factors = 1)
###
###factanal(x=data.pred.long[,c(3:514)],factors=1)

library(dplyr)
library(glmmLasso)
names(data.pred.long)
data.pred.long <- data.pred[,grepl( "feature|id|timepoint|physical" , names( data.pred ) )]
data.pred.long <- data.pred.long[,c(1:130,515)]
featuresFiltered <- NULL
for (k in 1:(sum(grepl( "feature" , names( data.pred.long ))))){
  data.sd.patient <- cbind(table(data.pred.long$id),tapply(data.pred.long[,c(k+2)],INDEX = data.pred.long$id , sd))
  if(sum(data.sd.patient[,2]==0,na.rm=T)==0){
    featuresFiltered <- c(featuresFiltered, names(data.pred.long[grepl( "feature" , names( data.pred.long ))])[k])
  }
}
FeaturesExcluded <- names(data.pred.long[grepl( "feature" , names( data.pred.long ))])[-which( names(data.pred.long[grepl( "feature" , names( data.pred.long ))])%in%featuresFiltered)]

print(paste(c('features removed due to lack of variations are:',FeaturesExcluded,concatenate=' ')))

formula <-formula(paste0('physical ~',paste( featuresFiltered ,collapse='+')))
data.pred.long$id <- factor(data.pred.long$id )
cross.validation <- cv.glmmLasso(fix = formula , rnd = list(id=~1), data = data.pred.long, 
family = gaussian(link = "identity"), kfold = 5, lambda.final = 'lambda.1se')

final.model <- glmmLasso(fix=formula, rnd=list(id=~1), data = data.pred.long, lambda =cross.validation$lambda.1se, family = gaussian(link="identity"))

final.model$coefficients
```

## role

```{r}

data.pred.long <- data.pred[,grepl( "feature|id|timepoint|role" , names( data.pred ) )]

formula <-formula(paste0('role ~',paste( featuresFiltered ,collapse='+')))
data.pred.long$id <- factor(data.pred.long$id )
cross-validation <- cv.glmmLasso(fix = formula , rnd = list(id=~1), data = data.pred.long, 
family = gaussian(link = "identity"), kfold = 5, lambda.final = 'lambda.1se')

final.model <- glmmLasso(fix=formula, rnd=list(id=~1), data = data.pred.long, lambda =lambda.1se, family = gaussian(link="identity"))

final.model$coefficients
```

## fatigue
```{r}


data.pred.long <- data.pred[,grepl( "feature|id|timepoint|fatigue" , names( data.pred ) )]

formula <-formula(paste0('fatigue ~',paste( featuresFiltered ,collapse='+')))
data.pred.long$id <- factor(data.pred.long$id )
cross-validation <- cv.glmmLasso(fix = formula , rnd = list(id=~1), data = data.pred.long, 
family = gaussian(link = "identity"), kfold = 5, lambda.final = 'lambda.1se')

final.model <- glmmLasso(fix=formula, rnd=list(id=~1), data = data.pred.long, lambda =lambda.1se, family = gaussian(link="identity"))

final.model$coefficients
```